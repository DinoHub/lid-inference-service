{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08df7e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.10.0)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.56.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (65.5.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa) (3.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa) (23.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa) (2.28.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72301010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "0.13.1+cu116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import librosa\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec2bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99b1fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Main prediction function '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from config import config, BaseConfig\n",
    "from typing import Any, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "class TweakedEncoderClassifier(EncoderClassifier):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def classify_batch(self, wavs, wav_lens=None):\n",
    "        \"\"\"Performs classification on the top of the encoded features.\n",
    "\n",
    "        It returns the posterior probabilities, the index and, if the label\n",
    "        encoder is specified it also the text label.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        wavs : torch.tensor\n",
    "            Batch of waveforms [batch, time, channels] or [batch, time]\n",
    "            depending on the model. Make sure the sample rate is fs=16000 Hz.\n",
    "        wav_lens : torch.tensor\n",
    "            Lengths of the waveforms relative to the longest one in the\n",
    "            batch, tensor of shape [batch]. The longest one should have\n",
    "            relative length 1.0 and others len(waveform) / max_length.\n",
    "            Used for ignoring padding.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out_prob\n",
    "            The log posterior probabilities of each class ([batch, N_class])\n",
    "        score:\n",
    "            It is the value of the log-posterior for the best class ([batch,])\n",
    "        index\n",
    "            The indexes of the best class ([batch,])\n",
    "        text_lab:\n",
    "            List with the text labels corresponding to the indexes.\n",
    "            (label encoder should be provided).\n",
    "        \"\"\"\n",
    "        emb = self.encode_batch(wavs, wav_lens)\n",
    "        out_prob = self.mods.classifier(emb).squeeze(1)\n",
    "\n",
    "        return out_prob\n",
    "    \n",
    "    def postproc(self, out_prob):\n",
    "        score, index = torch.max(out_prob, dim=-1)\n",
    "        text_lab = self.hparams.label_encoder.decode_torch(index)\n",
    "        return score, index, text_lab\n",
    "\n",
    "''' CPU/GPU Configurations '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = [0]  # use 0th CUDA device\n",
    "    ACCELERATOR = 'gpu'\n",
    "else:\n",
    "    DEVICE = 1\n",
    "    ACCELERATOR = 'cpu'\n",
    "\n",
    "MAP_LOCATION: str = torch.device('cuda:{}'.format(DEVICE[0]) if ACCELERATOR == 'gpu' else 'cpu')\n",
    "\n",
    "\n",
    "''' Helper functions '''\n",
    "def initialize_lid_model(cfg: BaseConfig) -> EncoderClassifier:\n",
    "\n",
    "    # lid_model = EncoderClassifier.from_hparams(source=cfg.model_source, savedir=cfg.model_dir)\n",
    "    lid_model = TweakedEncoderClassifier.from_hparams(source=cfg.model_source, savedir=cfg.model_source)\n",
    "\n",
    "    return lid_model\n",
    "\n",
    "''' Initialize models '''\n",
    "lid_model = initialize_lid_model(config)\n",
    "\n",
    "''' Main prediction function '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3af5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.rand([20800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0027e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6807, 0.2068, 0.5845,  ..., 0.6307, 0.5405, 0.2605])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914d6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction =  lid_model(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6500f3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 107])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3fc882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.7311, -5.6210, -6.5004, -5.5601, -6.3318, -5.7018, -6.3189, -6.8618,\n",
       "         -5.6906, -6.6859, -4.5642, -6.2262, -3.8397, -5.0002, -7.9657, -5.6553,\n",
       "         -1.3815, -6.5146, -7.1239, -6.1227, -3.6416, -7.9683, -5.4432, -6.2893,\n",
       "         -6.4234, -5.9083, -6.2119, -4.4752, -5.2745, -4.5266, -9.2858, -6.0738,\n",
       "         -7.3615, -5.2734, -6.4271, -5.2064, -3.6185, -4.7815, -5.5289, -6.2376,\n",
       "         -7.8543, -6.4990, -4.6146, -5.7218, -4.6755, -5.7481, -3.4200, -6.2221,\n",
       "         -5.9652, -5.6839, -7.2023, -6.9333, -3.7232, -6.4151, -6.2971, -5.2658,\n",
       "         -5.0486, -6.4945, -4.5951, -2.9194, -4.9518, -7.1848, -4.9657, -5.7736,\n",
       "         -3.2184, -5.9307, -7.2750, -8.4384, -6.1516, -2.4973, -3.3454, -5.1725,\n",
       "         -7.1693, -5.9150, -7.0843, -4.5789, -5.4315, -5.4546, -8.5199, -5.8809,\n",
       "         -2.5536, -7.1859, -5.2396, -4.2056, -5.2110, -5.0028, -4.9338, -5.9506,\n",
       "         -5.2701, -6.5793, -6.2318, -8.2121, -6.6880, -6.4567, -7.1406, -6.1185,\n",
       "         -5.1429, -6.3483, -6.7867, -7.7904, -8.1994, -7.2367, -6.1923, -4.5080,\n",
       "         -5.0182, -6.7661, -6.9789]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lid_model(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d16de22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.7311, -5.6210, -6.5004, -5.5601, -6.3318, -5.7018, -6.3189, -6.8618,\n",
       "         -5.6906, -6.6859, -4.5642, -6.2262, -3.8397, -5.0002, -7.9657, -5.6553,\n",
       "         -1.3815, -6.5146, -7.1239, -6.1227, -3.6416, -7.9683, -5.4432, -6.2893,\n",
       "         -6.4234, -5.9083, -6.2119, -4.4752, -5.2745, -4.5266, -9.2858, -6.0738,\n",
       "         -7.3615, -5.2734, -6.4271, -5.2064, -3.6185, -4.7815, -5.5289, -6.2376,\n",
       "         -7.8543, -6.4990, -4.6146, -5.7218, -4.6755, -5.7481, -3.4200, -6.2221,\n",
       "         -5.9652, -5.6839, -7.2023, -6.9333, -3.7232, -6.4151, -6.2971, -5.2658,\n",
       "         -5.0486, -6.4945, -4.5951, -2.9194, -4.9518, -7.1848, -4.9657, -5.7736,\n",
       "         -3.2184, -5.9307, -7.2750, -8.4384, -6.1516, -2.4973, -3.3454, -5.1725,\n",
       "         -7.1693, -5.9150, -7.0843, -4.5789, -5.4315, -5.4546, -8.5199, -5.8809,\n",
       "         -2.5536, -7.1859, -5.2396, -4.2056, -5.2110, -5.0028, -4.9338, -5.9506,\n",
       "         -5.2701, -6.5793, -6.2318, -8.2121, -6.6880, -6.4567, -7.1406, -6.1185,\n",
       "         -5.1429, -6.3483, -6.7867, -7.7904, -8.1994, -7.2367, -6.1923, -4.5080,\n",
       "         -5.0182, -6.7661, -6.9789]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lid_model.classify_batch(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383105fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20800])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['vi: Vietnamese'], 'Language')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predict import predict\n",
    "\n",
    "predict('../data/008605190016.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19b89b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(model, output_path):\n",
    "    input = torch.rand([20800])\n",
    "    output = model(input)\n",
    "#     print(output)\n",
    "    traced_model = torch.jit.trace(model, input)\n",
    "    output = traced_model(input)\n",
    "#     print(output)\n",
    "    return traced_model.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a4c3868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -8.4211,  -5.7684,  -6.5353,  -5.9944,  -6.6675,  -6.2393,  -6.8956,\n",
      "          -6.2319,  -5.3388,  -6.5591,  -5.3820,  -6.2806,  -4.5073,  -4.7015,\n",
      "          -8.8633,  -6.1905,  -1.2191,  -5.8663,  -6.6861,  -5.8478,  -4.1391,\n",
      "          -7.7261,  -6.2417,  -6.4681,  -7.2637,  -6.6452,  -6.2837,  -4.6377,\n",
      "          -5.7326,  -4.8048, -10.1575,  -6.5117,  -8.3743,  -5.4764,  -7.0779,\n",
      "          -4.9656,  -3.8062,  -5.2660,  -5.9786,  -7.1776,  -7.4311,  -6.6288,\n",
      "          -4.4689,  -5.3252,  -5.1155,  -6.2268,  -3.3577,  -6.2834,  -6.2473,\n",
      "          -6.5521,  -7.1720,  -7.5314,  -3.4090,  -6.3522,  -7.2313,  -5.5708,\n",
      "          -5.5663,  -6.6187,  -4.3495,  -3.4982,  -5.2640,  -7.3352,  -5.8228,\n",
      "          -4.6462,  -2.8901,  -5.5363,  -7.7914,  -8.5715,  -6.2589,  -2.4978,\n",
      "          -3.1449,  -5.6118,  -6.5843,  -5.7991,  -7.0528,  -5.7123,  -5.8994,\n",
      "          -5.1514,  -8.5538,  -6.8959,  -2.3273,  -7.0570,  -5.2221,  -4.3042,\n",
      "          -5.8285,  -5.4263,  -5.3495,  -7.1256,  -4.6671,  -6.8619,  -6.5082,\n",
      "          -8.2900,  -6.3679,  -5.8975,  -6.8843,  -6.8535,  -6.1082,  -6.2225,\n",
      "          -6.3526,  -7.0702,  -7.4317,  -7.0497,  -6.2973,  -4.3932,  -5.5410,\n",
      "          -7.0445,  -7.4394]])\n",
      "tensor([[ -8.4211,  -5.7684,  -6.5353,  -5.9944,  -6.6675,  -6.2393,  -6.8956,\n",
      "          -6.2319,  -5.3388,  -6.5591,  -5.3820,  -6.2806,  -4.5073,  -4.7015,\n",
      "          -8.8633,  -6.1905,  -1.2191,  -5.8663,  -6.6861,  -5.8478,  -4.1391,\n",
      "          -7.7261,  -6.2417,  -6.4681,  -7.2637,  -6.6452,  -6.2837,  -4.6377,\n",
      "          -5.7326,  -4.8048, -10.1575,  -6.5117,  -8.3743,  -5.4764,  -7.0779,\n",
      "          -4.9656,  -3.8062,  -5.2660,  -5.9786,  -7.1776,  -7.4311,  -6.6288,\n",
      "          -4.4689,  -5.3252,  -5.1155,  -6.2268,  -3.3577,  -6.2834,  -6.2473,\n",
      "          -6.5521,  -7.1720,  -7.5314,  -3.4090,  -6.3522,  -7.2313,  -5.5708,\n",
      "          -5.5663,  -6.6187,  -4.3495,  -3.4982,  -5.2640,  -7.3352,  -5.8228,\n",
      "          -4.6462,  -2.8901,  -5.5363,  -7.7914,  -8.5715,  -6.2589,  -2.4978,\n",
      "          -3.1449,  -5.6118,  -6.5843,  -5.7991,  -7.0528,  -5.7123,  -5.8994,\n",
      "          -5.1514,  -8.5538,  -6.8959,  -2.3273,  -7.0570,  -5.2221,  -4.3042,\n",
      "          -5.8285,  -5.4263,  -5.3495,  -7.1256,  -4.6671,  -6.8619,  -6.5082,\n",
      "          -8.2900,  -6.3679,  -5.8975,  -6.8843,  -6.8535,  -6.1082,  -6.2225,\n",
      "          -6.3526,  -7.0702,  -7.4317,  -7.0497,  -6.2973,  -4.3932,  -5.5410,\n",
      "          -7.0445,  -7.4394]])\n"
     ]
    }
   ],
   "source": [
    "out_status = trace(lid_model, 'test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9db407",
   "metadata": {},
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5cceba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "008605190016-0-100.wav\t__init__.py  flagged\t\t       test.pt\r\n",
      "008605190016.wav\tapp.py\t     lid-preproc.py\r\n",
      "DataLoader.ipynb\tconfig.py    speechbrain-triton.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
